{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# With regard to this paper:https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/ \n#a Researchers from the Oxford Visual Geometry Group, or VGG for short, participate in the ILSVRC challenge(Image Large Scale Visual Recognition Challenge)For the classification task, images must be classified into one of 1,000 different categories.\n#vgg16 image classification model\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('FMGahgc8wx0', width=800, height=450)\n#----------------------------------------------------------\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-01T21:28:03.099642Z","iopub.execute_input":"2022-03-01T21:28:03.099904Z","iopub.status.idle":"2022-03-01T21:28:03.182765Z","shell.execute_reply.started":"2022-03-01T21:28:03.099870Z","shell.execute_reply":"2022-03-01T21:28:03.182020Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#1-import requested libraries\nimport tensorflow as tf\nfrom tensorflow import keras\n# print(keras.__version__)\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:46.250092Z","iopub.execute_input":"2022-02-28T21:26:46.250340Z","iopub.status.idle":"2022-02-28T21:26:46.262737Z","shell.execute_reply.started":"2022-02-28T21:26:46.250312Z","shell.execute_reply":"2022-02-28T21:26:46.262036Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#2-loading data:\nimage=load_img('../input/image-test-cat/cat.png',target_size=(224,224))\n#----------------------------------------------------------\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:38:44.682306Z","iopub.execute_input":"2022-02-28T05:38:44.684940Z","iopub.status.idle":"2022-02-28T05:38:44.707007Z","shell.execute_reply.started":"2022-02-28T05:38:44.684874Z","shell.execute_reply":"2022-02-28T05:38:44.706054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#3-preprocessing data\n# converting the image to a numpy array\nimage=img_to_array(image)\n# 4-dimensional: samples(we have here one sample image so it's 1), rows, columns, and channels.\nimage=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n# the image pixels need to be prepared in the same way as the ImageNet training data was prepared. Specifically, from the paper:\nimage=preprocess_input(image)\n#----------------------------------------------------------\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:38:44.709777Z","iopub.execute_input":"2022-02-28T05:38:44.710295Z","iopub.status.idle":"2022-02-28T05:38:44.717912Z","shell.execute_reply.started":"2022-02-28T05:38:44.710250Z","shell.execute_reply":"2022-02-28T05:38:44.716653Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#4-building the model\nmodel=VGG16()\n# Keras will download the weight files from the Internet and store them in the ~/.keras/models directory.\n#Note that the weights are about 528 megabytes and Total params: 138,357,544Trainable params\n# ----------------------------------------------------------\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can also create a plot of the layers in the VGG model, as follows:\nfrom keras.utils.vis_utils import plot_model\nmodel=VGG16()\nplot_model(model, to_file='vgg.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5-predicting\n#to get a prediction of the probability of the image belonging to each of the 1000 known object types.\nyhat=model.predict(image)\n#----------------------------------------------------------\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6-decoding predictions (classifying based on the annotation label not on a number i.e mnist)\nlabel=decode_predictions(yhat)\n#----------------------------------------------------------\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:38:59.326167Z","iopub.execute_input":"2022-02-28T05:38:59.326511Z","iopub.status.idle":"2022-02-28T05:38:59.570223Z","shell.execute_reply.started":"2022-02-28T05:38:59.326457Z","shell.execute_reply":"2022-02-28T05:38:59.569288Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#7-retrieving the highest probability\nlabel=label[0][0]\n#----------------------------------------------------------\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:38:59.571794Z","iopub.execute_input":"2022-02-28T05:38:59.572144Z","iopub.status.idle":"2022-02-28T05:38:59.577594Z","shell.execute_reply.started":"2022-02-28T05:38:59.572102Z","shell.execute_reply":"2022-02-28T05:38:59.575792Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#8-printing the classification\nprint('%s()%.2f%%)'%(label[1],label[2]*100))\n# showing the tested image\nfrom IPython.display import Image\nImage('../input/image-test-cat/cat.png')\nImage('../input/image-test-cat/cat.png',width=224,height=224)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:38:59.579927Z","iopub.execute_input":"2022-02-28T05:38:59.580397Z","iopub.status.idle":"2022-02-28T05:38:59.600011Z","shell.execute_reply.started":"2022-02-28T05:38:59.580336Z","shell.execute_reply":"2022-02-28T05:38:59.599158Z"},"trusted":true},"execution_count":10,"outputs":[]}]}